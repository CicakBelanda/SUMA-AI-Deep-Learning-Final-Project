{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5333b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61652f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e662642",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"dataset\"\n",
    "\n",
    "cnn = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\":      f\"{root}/cnn/train/*.parquet\",\n",
    "        \"validation\": f\"{root}/cnn/validation/*.parquet\",\n",
    "        \"test\":       f\"{root}/cnn/test/*.parquet\", \n",
    "    }\n",
    ")\n",
    "train_dataset = cnn[\"train\"]\n",
    "val_dataset   = cnn[\"validation\"].select(range(2000))\n",
    "test_dataset  = cnn[\"test\"].select(range(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c445d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d591614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 768\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"highlights\"],\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in ids]\n",
    "        for ids in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_tokenized = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee095c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # ambil preds dari tuple jika perlu\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.clip(preds, a_min=0, a_max=None)\n",
    "\n",
    "    # ubah -100 ke pad_token_id supaya bisa didecode\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # decode\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # clean\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "\n",
    "    # hitung ROUGE\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # ubah skala ke persen (opsional)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "362a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_cache=False)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "output_dir = \"./bart_cnn_finetuned\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_cnn\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_rougeL\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf99d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14d1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True, sortish_sampler: bool = False, predict_with_generate: bool = False, generation_max_length: Optional[int] = None, generation_num_beams: Optional[int] = None, generation_config: Union[str, pathlib.Path, transformers.generation.configuration_utils.GenerationConfig, NoneType] = None) -> None\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import inspect\n",
    "print(inspect.signature(Seq2SeqTrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3045978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_29440\\1999168131.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032f251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71784' max='71784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71784/71784 14:27:06, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.019500</td>\n",
       "      <td>1.878123</td>\n",
       "      <td>35.825700</td>\n",
       "      <td>15.596600</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>32.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.833800</td>\n",
       "      <td>1.848299</td>\n",
       "      <td>35.567600</td>\n",
       "      <td>15.469000</td>\n",
       "      <td>25.761100</td>\n",
       "      <td>32.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.704400</td>\n",
       "      <td>1.852839</td>\n",
       "      <td>36.445200</td>\n",
       "      <td>15.984100</td>\n",
       "      <td>26.184200</td>\n",
       "      <td>33.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.628200</td>\n",
       "      <td>1.843191</td>\n",
       "      <td>37.105300</td>\n",
       "      <td>16.154000</td>\n",
       "      <td>26.377900</td>\n",
       "      <td>34.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.544900</td>\n",
       "      <td>1.853586</td>\n",
       "      <td>36.805700</td>\n",
       "      <td>15.840800</td>\n",
       "      <td>26.194500</td>\n",
       "      <td>33.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.488600</td>\n",
       "      <td>1.860694</td>\n",
       "      <td>36.594800</td>\n",
       "      <td>15.673700</td>\n",
       "      <td>26.026100</td>\n",
       "      <td>33.781600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\library\\environments\\text_mining\\lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=71784, training_loss=1.72105765207202, metrics={'train_runtime': 52027.7841, 'train_samples_per_second': 11.037, 'train_steps_per_second': 1.38, 'total_flos': 2.625967248113664e+17, 'train_loss': 1.72105765207202, 'epoch': 6.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8efb1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.35981904833861045, 'rouge2': 0.15595646040273925, 'rougeL': 0.2621508398425323, 'rougeLsum': 0.3320407701281053}\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "refs  = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    text   = sample[\"article\"]\n",
    "    target = sample[\"highlights\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=128,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    preds.append(summary)\n",
    "    refs.append(target)\n",
    "\n",
    "scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ca1c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\library\\environments\\text_mining\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./BART_NewsSummarizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint_path = \"./bart_cnn/checkpoint-71784\"\n",
    "save_path = \"./BART_NewsSummarizer\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"Model saved to:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
